# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nPXdJK9ltn03BeovjKean-XipITD3B0f
"""

# 1.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Create the Sequential model
model = Sequential([
    Flatten(input_shape=(28, 28)),              # Input layer
    Dense(128, activation='relu'),              # Hidden layer
    Dropout(0.3),                               # Dropout layer
    Dense(10, activation='softmax')             # Output layer
])

# Compile the model
model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Display the model architecture
model.summary()

# 2.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam

# Create the CNN model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),  # 1st Conv layer
    MaxPooling2D(pool_size=(2,2)),                                  # 1st Pooling layer
    Conv2D(64, (3,3), activation='relu'),                           # 2nd Conv layer
    Flatten(),                                                      # Flatten for dense layers
    Dense(128, activation='relu'),                                  # Hidden layer
    Dense(10, activation='softmax')                                 # Output layer (10 classes)
])

# Compile the model
model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Show model summary
model.summary()

# 3.

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Assume 'model' is your pre-trained Sequential model
# Let's modify it by adding a Dense layer before the output

# Get all layers
layers = model.layers

# Create a new Sequential model
new_model = Sequential()

# Add all previous layers except the last (output) layer
for layer in layers[:-1]:
    new_model.add(layer)

# Add the new Dense layer (128 neurons, ReLU) before the original output layer
new_model.add(Dense(128, activation='relu'))

# Add the old output layer again
new_model.add(layers[-1])


# Compile the new model
new_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

# Check summary
new_model.summary()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# ✅ Difference explained in comments:

# Sigmoid:
# - Used for binary classification (2 classes: e.g., cat vs. dog).
# - Outputs a probability between 0 and 1 for EACH neuron independently.
# - If you use multiple sigmoid neurons, each one acts as a separate binary classifier.
#   Example: multi-label classification (an image can be both "cat" and "pet").

# Softmax:
# - Used for multi-class classification (e.g., 3 or more classes like cat, dog, horse).
# - Outputs a probability distribution across all classes that sums to 1.
# - The class with the highest probability is the model’s prediction.

# ✅ Example: 3-class classification model using Softmax

model = Sequential([
    Dense(16, activation='relu', input_shape=(10,)),  # Input layer (10 features)
    Dense(8, activation='relu'),                      # Hidden layer
    Dense(3, activation='softmax')                    # Output: 3 classes → use Softmax
])

# Compile the model
model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',   # Used with softmax outputs
              metrics=['accuracy'])

# Show the model summary
model.summary()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras.optimizers import Adam

# Define two input layers
input1 = Input(shape=(32,), name='Input1')
input2 = Input(shape=(32,), name='Input2')

# Concatenate the two inputs
merged = Concatenate()([input1, input2])

# Hidden dense layer with 64 neurons and ReLU activation
hidden = Dense(64, activation='relu')(merged)

# Output layer with 1 neuron and sigmoid activation
output = Dense(1, activation='sigmoid')(hidden)

# Create the functional model
model = Model(inputs=[input1, input2], outputs=output)

# Compile the model
model.compile(optimizer=Adam(),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Display the model summary
model.summary()

from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical

# 1️⃣ Load CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# 2️⃣ Normalize image pixel values to range [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# 3️⃣ One-hot encode the labels (10 classes)
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

# 4️⃣ Display shapes to verify
print("Training data shape:", x_train.shape)
print("Training labels shape:", y_train.shape)
print("Test data shape:", x_test.shape)
print("Test labels shape:", y_test.shape)

import numpy as np

# Suppose X has shape (1000, 28, 28)
# 1000 images, each 28x28 pixels (grayscale)

# Define a sample X for demonstration
X = np.random.rand(1000, 28, 28)


# ✅ Reshape to include the channel dimension
X_reshaped = X.reshape((1000, 28, 28, 1))

# Optionally, convert to float and normalize to [0,1]
X_reshaped = X_reshaped.astype('float32') / 255.0

# Check the new shape
print("Original shape:", X.shape)
print("Reshaped for CNN:", X_reshaped.shape)

from sklearn.model_selection import train_test_split

# Example dataset (X = features, y = labels)
# Assume X and y are already defined, e.g. X.shape = (1000, 28, 28), y.shape = (1000,)
# Step 1: Split into train (70%) and temp (30%)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)

# Step 2: Split temp into validation (15%) and test (15%)
# Since X_temp is 30%, we split it equally into 15% each => test_size=0.5
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Check dataset sizes
print(f"Training set: {len(X_train)} samples")
print(f"Validation set: {len(X_val)} samples")
print(f"Testing set: {len(X_test)} samples")

from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np # Import numpy

# Create an ImageDataGenerator with the specified augmentations
datagen = ImageDataGenerator(
    rotation_range=15,          # Rotate images up to 15 degrees
    width_shift_range=0.1,      # Shift width by 10%
    height_shift_range=0.1,     # Shift height by 10%
    horizontal_flip=True,       # Randomly flip images horizontally
    fill_mode='nearest'         # Fill empty pixels after rotation/shift
)

# Example: assuming X_train contains your training images
# Reshape X_train to include a channel dimension (assuming grayscale)
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)

# Fit the generator on your data (optional but good for some datasets)
datagen.fit(X_train_reshaped)

# Example of using the generator in training
# model.fit(datagen.flow(X_train_reshaped, y_train, batch_size=32), epochs=20)

import matplotlib.pyplot as plt
import numpy as np

# Assume X contains images and y contains labels
# For example, X.shape = (1000, 28, 28, 1) or (1000, 28, 28, 3)
# y can be numeric labels or one-hot encoded (if one-hot, convert to class index)
if len(y.shape) > 1:  # one-hot encoded
    y_labels = np.argmax(y, axis=1)
else:
    y_labels = y

# Randomly select 5 indices
random_indices = np.random.choice(len(X), size=5, replace=False)

# Plot the images with labels
plt.figure(figsize=(12, 3))
for i, idx in enumerate(random_indices):
    plt.subplot(1, 5, i + 1)
    # Squeeze in case of single channel (grayscale)
    if X.shape[-1] == 1:
        plt.imshow(X[idx].squeeze(), cmap='gray')
    else:
        plt.imshow(X[idx])
    plt.title(f"Label: {y_labels[idx]}")
    plt.axis('off')

plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping
import numpy as np # Import numpy

# Define sample data for demonstration
X_train = np.random.rand(700, 28, 28, 1) # Sample training images (700 samples, 28x28 grayscale)
y_train = np.random.randint(0, 10, 700) # Sample training labels (700 samples, numeric)
X_val = np.random.rand(150, 28, 28, 1)   # Sample validation images (150 samples, 28x28 grayscale)
y_val = np.random.randint(0, 10, 150)   # Sample validation labels (150 samples, numeric)

# One-hot encode the labels (if not already done)
from tensorflow.keras.utils import to_categorical
y_train = to_categorical(y_train, num_classes=10)
y_val = to_categorical(y_val, num_classes=10)

# Assuming y_train is one-hot encoded, determine the number of classes
num_classes = y_train.shape[1]

# Reshape X_train to include a channel dimension if it's missing (assuming grayscale)
if len(X_train.shape) == 3:
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)
if len(X_val.shape) == 3:
    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], 1)


# Example Sequential model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=X_train.shape[1:]),
    MaxPooling2D((2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(num_classes, activation='softmax')  # num_classes = number of classes in y
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# EarlyStopping callback: stop if validation loss doesn't improve for 3 epochs
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[early_stop]
)

from tensorflow.keras.callbacks import ModelCheckpoint

# Create a ModelCheckpoint callback
checkpoint = ModelCheckpoint(
    'best_model.h5',          # Filepath to save the model
    monitor='val_accuracy',   # Metric to monitor
    save_best_only=True,      # Save only the best model
    mode='max',               # 'max' because higher val_accuracy is better
    verbose=1
)

# Train the model with checkpoint (assuming 'model' is already defined)
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_val, y_val),
    callbacks=[checkpoint]   # Include ModelCheckpoint callback
)

import matplotlib.pyplot as plt

# Assume 'history' is the History object returned by model.fit()
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)

# Plot training and validation loss
plt.figure(figsize=(8,5))
plt.plot(epochs, loss, 'b-', label='Training Loss')
plt.plot(epochs, val_loss, 'r-', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from tensorflow.keras.utils import to_categorical
import numpy as np # Import numpy

# Define sample data for demonstration
X_test = np.random.rand(150, 28, 28, 1) # Sample test images (150 samples, 28x28 grayscale)
y_test = np.random.randint(0, 10, 150)   # Sample test labels (150 samples, numeric)


# One-hot encode y_test if it's not already
if len(y_test.shape) == 1:
    y_test = to_categorical(y_test, num_classes=10) # Assuming 10 classes

# Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=32)

# Print the results
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

train_loss = [0.8, 0.5, 0.3, 0.2]
val_loss = [0.9, 0.6, 0.4, 0.5]

# Detect overfitting
overfitting = False
for i in range(1, len(train_loss)):
    if train_loss[i] < train_loss[i-1] and val_loss[i] > val_loss[i-1]:
        overfitting = True
        print(f"Overfitting detected at epoch {i+1}")
        break

if not overfitting:
    print("No overfitting detected")

from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

# Load a pre-trained CNN (e.g., VGG16) without top layer
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze all layers in the base model
for layer in base_model.layers:
    layer.trainable = False

# Add a new Dense output layer
x = base_model.output
x = Flatten()(x)
output = Dense(10, activation='softmax')(x)  # Example: 10 classes

# Create the final model
model = Model(inputs=base_model.input, outputs=output)

# Only the last Dense layer is trainable
for layer in model.layers:
    print(layer.name, layer.trainable)  # Optional: check which layers are trainable

# Compile the model for fine-tuning
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

print("Model ready for fine-tuning!")

import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# Example model
inputs = Input(shape=(20,))
x = Dense(32, activation='relu', name='dense1')(inputs)
x = Dense(16, activation='relu', name='dense2')(x)  # Second Dense layer
output = Dense(10, activation='softmax', name='output')(x)

model = Model(inputs=inputs, outputs=output)

# Single input sample
sample_input = np.random.rand(1, 20)  # Shape: (1, 20)

# Create a new model to output the second Dense layer
intermediate_layer_model = Model(inputs=model.input,
                                 outputs=model.get_layer('dense2').output)

# Get the output of the second Dense layer
intermediate_output = intermediate_layer_model.predict(sample_input)

print("Output of the second Dense layer:\n", intermediate_output)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# 1. Define the custom MSE loss function
def custom_mse(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

# 2. Create a simple model
model = Sequential([
    Dense(32, activation='relu', input_shape=(10,)),
    Dense(1)
])

# 3. Compile the model using the custom loss
model.compile(optimizer='adam', loss=custom_mse, metrics=['mae'])

# 4. Summary
model.summary()

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import Dense
import numpy as np

# ------------------------------
# 1️⃣ Create sample data
# ------------------------------
X = np.random.rand(100, 10)
y = np.random.rand(100, 1)

# ------------------------------
# 2️⃣ Create and train a simple model
# ------------------------------
model = Sequential([
    Dense(32, activation='relu', input_shape=(10,)),
    Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=2, batch_size=10, verbose=1)

# ------------------------------
# 3️⃣ Save the model
# ------------------------------
# Keras native format (recommended)
model.save('my_model.keras')

# SavedModel format
model.export('my_model_saved')

# ------------------------------
# 4️⃣ Load the model back
# ------------------------------
# From Keras native format
model_keras = load_model('my_model.keras')
print("Keras native format model loaded successfully!")

# From SavedModel
# Use TFSMLayer to load the SavedModel in Keras 3
loaded_saved_model_layer = tf.keras.layers.TFSMLayer('my_model_saved', call_endpoint='serving_default')

# To use it for prediction, you can wrap it in a Model
input_layer = tf.keras.Input(shape=(10,)) # Define input shape based on your model
model_saved = Model(inputs=input_layer, outputs=loaded_saved_model_layer(input_layer))

print("SavedModel format model loaded successfully using TFSMLayer!")


# ------------------------------
# 5️⃣ Check predictions are consistent
# ------------------------------
sample_input = np.random.rand(1, 10)

pred_original = model.predict(sample_input)
pred_keras = model_keras.predict(sample_input)
pred_saved = model_saved.predict(sample_input)


print("\nOriginal model prediction:", pred_original)
print("Keras loaded model prediction:", pred_keras)
print("SavedModel loaded model prediction:", pred_saved)

import numpy as np
import tensorflow as tf

# logits
logits = np.array([2.0, 1.0, 0.1])

# --- Manual softmax ---
exp_logits = np.exp(logits)
softmax_manual = exp_logits / np.sum(exp_logits)

# --- TensorFlow softmax ---
logits_tf = tf.constant([2.0, 1.0, 0.1])
softmax_tf = tf.nn.softmax(logits_tf)

# --- Print results ---
print("Manual softmax:", softmax_manual)
print("TensorFlow softmax:", softmax_tf.numpy())